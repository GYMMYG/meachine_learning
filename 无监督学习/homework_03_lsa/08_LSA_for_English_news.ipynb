{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所用的库\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "3387 documents\n",
      "4 categories\n"
     ]
    }
   ],
   "source": [
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 2 1 1]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "print(labels)\n",
    "print(true_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "From: healta@saturn.wwc.edu (Tammy R Healy)\n",
      "Subject: Re: who are we to judge, Bobby?\n",
      "Lines: 38\n",
      "Organization: Walla Walla College\n",
      "Lines: 38\n",
      "\n",
      "In article <1993Apr14.213356.22176@ultb.isc.rit.edu> snm6394@ultb.isc.rit.edu (S.N. Mozumder ) writes:\n",
      ">From: snm6394@ultb.isc.rit.edu (S.N. Mozumder )\n",
      ">Subject: Re: who are we to judge, Bobby?\n",
      ">Date: Wed, 14 Apr 1993 21:33:56 GMT\n",
      ">In article <healta.56.734556346@saturn.wwc.edu> healta@saturn.wwc.edu (TAMMY R HEALY) writes:\n",
      ">>Bobby,\n",
      ">>\n",
      ">>I would like to take the liberty to quote from a Christian writer named \n",
      ">>Ellen G. White.  I hope that what she said will help you to edit your \n",
      ">>remarks in this group in the future.\n",
      ">>\n",
      ">>\"Do not set yourself as a standard.  Do not make your opinions, your views \n",
      ">>of duty, your interpretations of scripture, a criterion for others and in \n",
      ">>your heart condemn them if they do not come up to your ideal.\"\n",
      ">>                         Thoughts Fromthe Mount of Blessing p. 124\n",
      ">>\n",
      ">>I hope quoting this doesn't make the atheists gag, but I think Ellen White \n",
      ">>put it better than I could.\n",
      ">> \n",
      ">>Tammy\n",
      ">\n",
      ">Point?\n",
      ">\n",
      ">Peace,\n",
      ">\n",
      ">Bobby Mozumder\n",
      ">\n",
      "My point is that you set up your views as the only way to believe.  Saying \n",
      "that all eveil in this world is caused by atheism is ridiculous and \n",
      "counterproductive to dialogue in this newsgroups.  I see in your posts a \n",
      "spirit of condemnation of the atheists in this newsgroup bacause they don'\n",
      "t believe exactly as you do.  If you're here to try to convert the atheists \n",
      "here, you're failing miserably.  Who wants to be in position of constantly \n",
      "defending themselves agaist insulting attacks, like you seem to like to do?!\n",
      "I'm sorry you're so blind that you didn't get the messgae in the quote, \n",
      "everyone else has seemed to.\n",
      "\n",
      "Tammy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset.data))\n",
    "print(dataset.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features ......\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features ......\")\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.708865s\n",
      "n_samples: 3387, n_features: 10000\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n",
    "                                 min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2825)\t0.04520567479618447\n",
      "  (0, 1426)\t0.07575320795576096\n",
      "  (0, 8349)\t0.04998796203184946\n",
      "  (0, 1109)\t0.08455246817382248\n",
      "  (0, 4733)\t0.08455246817382248\n",
      "  (0, 2658)\t0.08226616850753836\n",
      "  (0, 2243)\t0.08031575260409271\n",
      "  (0, 6957)\t0.05477744646671208\n",
      "  (0, 9681)\t0.06044054061483994\n",
      "  (0, 3507)\t0.08953739821545949\n",
      "  (0, 2320)\t0.05831236733334342\n",
      "  (0, 9184)\t0.04404807348832485\n",
      "  (0, 3372)\t0.054255653220676714\n",
      "  (0, 2984)\t0.02845277773309223\n",
      "  (0, 6178)\t0.06099495510090287\n",
      "  (0, 2190)\t0.09080540698179555\n",
      "  (0, 8433)\t0.07079512761524998\n",
      "  (0, 6971)\t0.06071486007349951\n",
      "  (0, 6179)\t0.06836951754105432\n",
      "  (0, 2815)\t0.08953739821545949\n",
      "  (0, 7705)\t0.080934655933792\n",
      "  (0, 1084)\t0.05326971909969163\n",
      "  (0, 1769)\t0.0587833553635613\n",
      "  (0, 9874)\t0.033418771247006686\n",
      "  (0, 7915)\t0.048748109937171095\n",
      "  :\t:\n",
      "  (3386, 6719)\t0.04636574358198852\n",
      "  (3386, 207)\t0.073055490407179\n",
      "  (3386, 8926)\t0.058413220065501995\n",
      "  (3386, 6135)\t0.05509573409226583\n",
      "  (3386, 5416)\t0.12261715332194234\n",
      "  (3386, 4674)\t0.15084827614247748\n",
      "  (3386, 2166)\t0.11482161605032963\n",
      "  (3386, 5091)\t0.06381270757122054\n",
      "  (3386, 5394)\t0.15310799606496223\n",
      "  (3386, 5453)\t0.06423374636793071\n",
      "  (3386, 8402)\t0.08300888851372776\n",
      "  (3386, 300)\t0.08350610582125675\n",
      "  (3386, 6490)\t0.07315255477001704\n",
      "  (3386, 6248)\t0.17522991214494527\n",
      "  (3386, 4676)\t0.12498320499549762\n",
      "  (3386, 6965)\t0.06049227697380511\n",
      "  (3386, 2114)\t0.1670122116425135\n",
      "  (3386, 8322)\t0.2488647336670735\n",
      "  (3386, 4095)\t0.24692113531279744\n",
      "  (3386, 4418)\t0.034286035217165646\n",
      "  (3386, 6969)\t0.033212553876237796\n",
      "  (3386, 6224)\t0.03451329591620291\n",
      "  (3386, 8852)\t0.06669826131063836\n",
      "  (3386, 9339)\t0.07501265631943817\n",
      "  (3386, 6450)\t0.07488347353395008\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction using LSA......\n",
      "done in 14.098891s\n",
      "Explained variance of the SVD step: 81%\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing dimensionality reduction using SVD......\")\n",
    "t0 = time()\n",
    "svd = TruncatedSVD(n_components = 1200)\n",
    "X = svd.fit_transform(X)\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3387, 1200)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do clustering......\n",
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "       n_clusters=4, n_init=1, n_jobs=None, precompute_distances='auto',\n",
      "       random_state=None, tol=0.0001, verbose=0)\n",
      "done in 1.270s\n"
     ]
    }
   ],
   "source": [
    "print(\"Do clustering......\")\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.455\n",
      "Completeness: 0.539\n",
      "V-measure: 0.493\n",
      "Adjusted Rand-Index: 0.394\n",
      "Silhouette Coefficient: 0.009\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0: com graphics university posting host nntp image thanks computer know\n",
      "Cluster 1: henry access toronto digex pat zoo spencer net zoology prb\n",
      "Cluster 2: god com people sandvik keith jesus don article say morality\n",
      "Cluster 3: space nasa gov alaska shuttle moon launch jpl just station\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
