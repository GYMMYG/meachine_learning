{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大数据管理作业04_PLSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "郭英明 2183211376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLSA:\n",
    "    '''\n",
    "    model:PLSA(EM)\n",
    "    k:话题数\n",
    "    language：文本语言\n",
    "    method:  1:生成，2：共现 \n",
    "    '''\n",
    "    def __init__(self,text_list,k,language,method = 1):\n",
    "        self.k = k\n",
    "        self.text_list = text_list\n",
    "        self.text_num = len(text_list)\n",
    "        self.method = method\n",
    "        self.get_X(language)\n",
    "\n",
    "    def get_X(self,language):\n",
    "        if language == 'chinese':\n",
    "            self.cuted_text = [jieba.lcut(text,cut_all=True) for text in self.text_list]\n",
    "            \n",
    "        if language == 'english':\n",
    "            news_df = pd.DataFrame({'document':self.text_list})\n",
    "            news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "            news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "            news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "            stop_words = stopwords.words('english')\n",
    "            tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "            self.cuted_text = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "            \n",
    "        self.word_all = []\n",
    "        for i in self.cuted_text:\n",
    "            self.word_all.extend(i)   \n",
    "            \n",
    "        self.word_set = list(set(self.word_all))\n",
    "        self.word_num = len(self.word_set)\n",
    "        self.word_dict = {}\n",
    "        for index,word in enumerate(self.word_set):\n",
    "            self.word_dict[word] = index\n",
    "        self.X = np.zeros((self.word_num,self.text_num))\n",
    "        for i in range(self.text_num):\n",
    "            count_ = collections.Counter(self.cuted_text[i])\n",
    "            for k, v in count_.items():\n",
    "                self.X[self.word_dict[k],i] = v\n",
    "\n",
    "\n",
    "    def shengcheng(self,max_iter):\n",
    "        self.initial_1()       \n",
    "        for iter in range(max_iter):\n",
    "            self.update_E_1()\n",
    "            self.update_M_1()\n",
    "            \n",
    "    def initial_1(self):\n",
    "        self.w_z  = np.random.random((self.word_num,self.k))\n",
    "        self.z_d = np.random.random((self.k,self.text_num))\n",
    "        \n",
    "    def update_E_1(self):\n",
    "        self.z_wd  = np.zeros((self.word_num,self.text_num,self.k))\n",
    "        for i in range(self.word_num):\n",
    "            for j in range(self.text_num):\n",
    "                self.z_wd[i,j] = np.array([self.w_z[i]*self.z_d[:,j]]) / np.sum([self.w_z[i]*self.z_d[:,j]])\n",
    "    \n",
    "    def update_M_1(self):\n",
    "        for k in range(self.k):\n",
    "            for i in range(self.word_num):\n",
    "                self.w_z[i,k] = np.sum(self.X[i]*self.z_wd[i,:,k])/\\\n",
    "                np.sum(self.X*self.z_wd[:,:,k])\n",
    "            for j in range(self.text_num):\n",
    "                self.z_d[k,j] = np.sum(self.X[:,j]*self.z_wd[:,j,k])/np.sum(self.X[:,j])\n",
    "      \n",
    "    \n",
    "    def gongxian(self,max_iter):\n",
    "        self.initial_2()       \n",
    "        for iter in range(max_iter):\n",
    "            self.update_E_2()\n",
    "            self.update_M_2()\n",
    "    \n",
    "    def initial_2(self):\n",
    "        self.w_z  = np.random.random((self.k,self.word_num))\n",
    "        self.d_z = np.random.random((self.k,self.text_num))\n",
    "        self.z = np.random.random((1,self.k))\n",
    "    \n",
    "    def update_E_2(self):\n",
    "#         self.z_wd  = np.zeros((self.word_num,self.text_num,self.k))\n",
    "#         for i in range(self.word_num):\n",
    "#             for j in range(self.text_num):\n",
    "#                 self.z_wd[i,j] = np.array([self.w_z[i]*self.z_d[:,j]]) / np.sum([self.w_z[i]*self.z_d[:,j]])\n",
    "        self.z_wd  = np.zeros((self.word_num,self.text_num,self.k))\n",
    "        for i in range(self.word_num):\n",
    "            for j in range(self.text_num):\n",
    "                self.z_wd[i,j] = np.array([self.w_z[:,i]*self.d_z[:,j]*self.z[0]]) / np.sum([self.w_z[:,i]*self.d_z[:,j]*self.z[0]])\n",
    "        \n",
    "    \n",
    "    def update_M_2(self):\n",
    "        for k in range(self.k):\n",
    "            for i in range(self.word_num):\n",
    "                self.w_z[k,i] = np.sum(self.X[i]*self.z_wd[i,:,k])/\\\n",
    "                np.sum(self.X*self.z_wd[:,:,k])\n",
    "            for j in range(self.text_num):\n",
    "                self.d_z[k,j] = np.sum(self.X[:,j]*self.z_wd[:,j,k])/np.sum(self.X[:,j])\n",
    "            self.z[0] = np.sum(self.X*self.z_wd[:,:,k]) / np.sum(self.X)\n",
    "    \n",
    "    def fit(self,max_iter):\n",
    "        if self.method == 1:\n",
    "            self.shengcheng(max_iter)\n",
    "        else:\n",
    "            self.gongxian(max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Scikit-learn库中导入新闻文本数据集(fetch_20newsgroups)的前10条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.62850821e-07 1.50850220e-03]\n",
      " [3.47729525e-05 1.48682675e-03]\n",
      " [2.40885082e-03 3.85926282e-10]\n",
      " ...\n",
      " [6.22685736e-07 1.50821421e-03]\n",
      " [8.73520607e-07 1.50805712e-03]\n",
      " [6.31287669e-25 1.50860419e-03]]\n",
      "[[7.45291538e-01 3.22796470e-03 5.73011770e-07 3.14248184e-02\n",
      "  9.98675965e-01 9.99999980e-01 1.15790050e-04 9.99999999e-01\n",
      "  2.70815136e-01 1.00000000e+00]\n",
      " [2.54708462e-01 9.96772035e-01 9.99999427e-01 9.68575182e-01\n",
      "  1.32403534e-03 2.02159695e-08 9.99884210e-01 5.39113604e-10\n",
      "  7.29184864e-01 6.37492650e-12]]\n"
     ]
    }
   ],
   "source": [
    "all_data = fetch_20newsgroups(subset='all')\n",
    "data = all_data.data[:10]\n",
    "# print(type(data))\n",
    "lsa1 = PLSA(data,k=2,language = 'english')\n",
    "lsa1.fit(10)\n",
    "print(lsa1.w_z)\n",
    "print(lsa1.z_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.64726386 0.64726386]]\n",
      "[[1.63860914e-31 1.25123659e-06 1.16741506e-24 ... 2.48603069e-31\n",
      "  6.93744227e-33 1.85175567e-11]\n",
      " [1.43317716e-03 1.43249528e-03 1.43317716e-03 ... 1.43317716e-03\n",
      "  1.43317716e-03 1.43317715e-03]]\n",
      "[[3.44560128e-01 9.99999894e-01 2.81121601e-04 5.86423719e-08\n",
      "  5.62375115e-06 8.28131825e-01 7.15545504e-01 9.94091268e-01\n",
      "  1.38825537e-01 9.99999988e-01]\n",
      " [6.55439872e-01 1.05712228e-07 9.99718878e-01 9.99999941e-01\n",
      "  9.99994376e-01 1.71868175e-01 2.84454496e-01 5.90873228e-03\n",
      "  8.61174463e-01 1.18310526e-08]]\n"
     ]
    }
   ],
   "source": [
    "lsa2 = PLSA(data,k=2,language = 'english',method = 2)\n",
    "lsa2.fit(10)\n",
    "print(lsa2.z)\n",
    "print(lsa2.w_z)\n",
    "print(lsa2.d_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
